{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-18T15:38:11.196624685Z",
     "start_time": "2023-07-18T15:38:11.196030463Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aris/projects/GraB-lib\n"
     ]
    }
   ],
   "source": [
    "%cd ~/projects/GraB-lib\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from functools import partial, reduce\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from absl import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from transformers import (\n",
    "    HfArgumentParser, TrainingArguments, set_seed\n",
    ")\n",
    "\n",
    "import torchopt\n",
    "from torch.func import (\n",
    "    grad, grad_and_value, vmap, functional_call\n",
    ")\n",
    "\n",
    "from grabngo import GraBSampler, BalanceType\n",
    "from grabngo.utils import EventTimer, pretty_time"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T15:38:14.285399437Z",
     "start_time": "2023-07-18T15:38:11.196544128Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "batch_size = 16\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        (0.1307,), (0.3081,)\n",
    "    ),\n",
    "    transforms.Lambda(lambda x: x.view(-1)),\n",
    "])\n",
    "\n",
    "# Loading the dataset and preprocessing\n",
    "train_dataset = datasets.MNIST(\n",
    "    root='data/external',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "# train_dataset = torch.utils.data.Subset(train_dataset, range(0, 6000))\n",
    "test_dataset = datasets.MNIST(\n",
    "    root='data/external',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "in_dim, num_classes = 784, 10\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    # sampler=sampler,\n",
    "    persistent_workers=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=True\n",
    ")\n",
    "train_eval_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    persistent_workers=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    persistent_workers=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=True\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T15:38:14.508522610Z",
     "start_time": "2023-07-18T15:38:14.288467839Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nLinear                                   [16, 10]                  7,850\n==========================================================================================\nTotal params: 7,850\nTrainable params: 7,850\nNon-trainable params: 0\nTotal mult-adds (M): 0.13\n==========================================================================================\nInput size (MB): 0.05\nForward/backward pass size (MB): 0.00\nParams size (MB): 0.03\nEstimated Total Size (MB): 0.08\n=========================================================================================="
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# model = nn.Sequential(\n",
    "#     nn.Linear(in_dim, 100),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(100, 100),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(100, num_classes)\n",
    "# )\n",
    "\n",
    "model = nn.Linear(in_dim, num_classes).to(device)\n",
    "\n",
    "summary(model, input_size=(batch_size, in_dim), device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T15:38:14.636703576Z",
     "start_time": "2023-07-18T15:38:14.508902582Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for epoch in range(1):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def make_functional(mod, disable_autograd_tracking=False):\n",
    "    params_dict = dict(mod.named_parameters())\n",
    "    params_names = params_dict.keys()\n",
    "    params_values = tuple(params_dict.values())\n",
    "\n",
    "    stateless_mod = copy.deepcopy(mod)\n",
    "    stateless_mod.to('meta')\n",
    "\n",
    "    def fmodel(new_params_values, *args, **kwargs):\n",
    "        new_params_dict = {name: value for name, value in zip(params_names, new_params_values)}\n",
    "        return torch.func.functional_call(stateless_mod, new_params_dict, args, kwargs)\n",
    "\n",
    "    if disable_autograd_tracking:\n",
    "        params_values = torch.utils._pytree.tree_map(torch.Tensor.detach, params_values)\n",
    "    return fmodel, params_values\n",
    "\n",
    "def make_functional_with_buffers(mod, disable_autograd_tracking=False):\n",
    "    params_dict = dict(mod.named_parameters())\n",
    "    params_names = params_dict.keys()\n",
    "    params_values = tuple(params_dict.values())\n",
    "\n",
    "    buffers_dict = dict(mod.named_buffers())\n",
    "    buffers_names = buffers_dict.keys()\n",
    "    buffers_values = tuple(buffers_dict.values())\n",
    "\n",
    "    stateless_mod = copy.deepcopy(mod)\n",
    "    stateless_mod.to('meta')\n",
    "\n",
    "    def fmodel(new_params_values, new_buffers_values, *args, **kwargs):\n",
    "        new_params_dict = {name: value for name, value in zip(params_names, new_params_values)}\n",
    "        new_buffers_dict = {name: value for name, value in zip(buffers_names, new_buffers_values)}\n",
    "        return torch.func.functional_call(stateless_mod, (new_params_dict, new_buffers_dict), args, kwargs)\n",
    "\n",
    "    if disable_autograd_tracking:\n",
    "        params_values = torch.utils._pytree.tree_map(torch.Tensor.detach, params_values)\n",
    "    return fmodel, params_values, buffers_values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T15:32:59.282975985Z",
     "start_time": "2023-07-18T15:32:59.240800122Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/47 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8dd62c7053ec4e1894ccd3d361a43535"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grabngo.sorter.beta import NTKBalance\n",
    "\n",
    "params = dict(model.named_parameters())\n",
    "buffers = dict(model.named_buffers())\n",
    "\n",
    "n = 6000\n",
    "d = sum(p.numel() for p in params.values())\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "data = train_dataset.data[:n].reshape(n, -1).float() / 255\n",
    "target = train_dataset.targets[:n]\n",
    "\n",
    "sorter = NTKBalance(\n",
    "    n, d, model, params, buffers, loss_fn, data, target\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T15:39:25.586148426Z",
     "start_time": "2023-07-18T15:39:25.435969637Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(6000, device='cuda:0')"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = sorter.K\n",
    "\n",
    "torch.linalg.matrix_rank(K, atol=1e-8, hermitian=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T15:42:35.337965898Z",
     "start_time": "2023-07-18T15:42:34.767899931Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 ms Â± 15.1 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit torch.linalg.eigh(K)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T15:47:57.759116477Z",
     "start_time": "2023-07-18T15:47:53.573933261Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([44706.7305], device='cuda:0', grad_fn=<LOBPCGAutogradFunctionBackward>),\n tensor([[ 0.0009],\n         [-0.0405],\n         [ 0.0007],\n         ...,\n         [ 0.0057],\n         [ 0.0023],\n         [ 0.0036]], device='cuda:0', grad_fn=<LOBPCGAutogradFunctionBackward>))"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.lobpcg(K, largest=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T15:56:35.028880867Z",
     "start_time": "2023-07-18T15:56:34.965739771Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(44706.7891, device='cuda:0', grad_fn=<MaxBackward1>)"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linalg.eigvalsh(K)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T15:50:03.254933713Z",
     "start_time": "2023-07-18T15:50:02.670434001Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "\u001B[31mâ­â\u001B[0m\u001B[31mââââââââââââââââââââââââââââââ\u001B[0m\u001B[31m \u001B[0m\u001B[1;31mTraceback \u001B[0m\u001B[1;2;31m(most recent call last)\u001B[0m\u001B[31m \u001B[0m\u001B[31mâââââââââââââââââââââââââââââââ\u001B[0m\u001B[31mââ®\u001B[0m\n\u001B[31mâ\u001B[0m in \u001B[92m<module>\u001B[0m:\u001B[94m85\u001B[0m                                                                                   \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m82 \u001B[0m\u001B[2mâ   â   \u001B[0m\u001B[96mprint\u001B[0m(\u001B[33m\"\u001B[0m\u001B[33mFailed to load data at once, loading data one by one\u001B[0m\u001B[33m\"\u001B[0m)                       \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m83 \u001B[0m\u001B[2mâ   â   \u001B[0m...                                                                                 \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m84 \u001B[0m                                                                                            \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[31mâ± \u001B[0m85 get_0shot_order(model, params, train_dataset)                                               \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m86 \u001B[0m                                                                                            \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[2;33m/home/aris/mambaforge/envs/grab/lib/python3.10/site-packages/torch/utils/\u001B[0m\u001B[1;33m_contextlib.py\u001B[0m:\u001B[94m115\u001B[0m in   \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[92mdecorate_context\u001B[0m                                                                                 \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m112 \u001B[0m\u001B[2mâ   \u001B[0m\u001B[1;95m@functools\u001B[0m.wraps(func)                                                                 \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m113 \u001B[0m\u001B[2mâ   \u001B[0m\u001B[94mdef\u001B[0m \u001B[92mdecorate_context\u001B[0m(*args, **kwargs):                                                 \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m114 \u001B[0m\u001B[2mâ   â   \u001B[0m\u001B[94mwith\u001B[0m ctx_factory():                                                                \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[31mâ± \u001B[0m115 \u001B[2mâ   â   â   \u001B[0m\u001B[94mreturn\u001B[0m func(*args, **kwargs)                                                   \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m116 \u001B[0m\u001B[2mâ   \u001B[0m                                                                                       \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m117 \u001B[0m\u001B[2mâ   \u001B[0m\u001B[94mreturn\u001B[0m decorate_context                                                                \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m118 \u001B[0m                                                                                           \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m in \u001B[92mget_0shot_order\u001B[0m:\u001B[94m81\u001B[0m                                                                            \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m78 \u001B[0m\u001B[2mâ   â   \u001B[0m                                                                                    \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m79 \u001B[0m\u001B[2mâ   â   \u001B[0m\u001B[96mprint\u001B[0m(ntk.shape)                                                                    \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m80 \u001B[0m\u001B[2mâ   \u001B[0m\u001B[94mexcept\u001B[0m torch.cuda.OutOfMemoryError \u001B[94mas\u001B[0m err:                                              \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[31mâ± \u001B[0m81 \u001B[2mâ   â   \u001B[0m\u001B[94mraise\u001B[0m err                                                                           \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m82 \u001B[0m\u001B[2mâ   â   \u001B[0m\u001B[96mprint\u001B[0m(\u001B[33m\"\u001B[0m\u001B[33mFailed to load data at once, loading data one by one\u001B[0m\u001B[33m\"\u001B[0m)                       \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m83 \u001B[0m\u001B[2mâ   â   \u001B[0m...                                                                                 \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m84 \u001B[0m                                                                                            \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m in \u001B[92mget_0shot_order\u001B[0m:\u001B[94m75\u001B[0m                                                                            \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m72 \u001B[0m\u001B[2mâ   â   \u001B[0mx = dataset.data.to(device=device).reshape(-\u001B[94m1\u001B[0m, \u001B[94m784\u001B[0m)  \u001B[2m# (n, ...)\u001B[0m                     \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m73 \u001B[0m\u001B[2mâ   â   \u001B[0mx = x.float() / \u001B[94m255\u001B[0m                                                                 \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m74 \u001B[0m\u001B[2mâ   â   \u001B[0m                                                                                    \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[31mâ± \u001B[0m75 \u001B[2mâ   â   \u001B[0mntk = empirical_ntk_ntk_vps(                                                        \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m76 \u001B[0m\u001B[2mâ   â   â   \u001B[0mfnet_single, params, x, x                                                       \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m77 \u001B[0m\u001B[2mâ   â   \u001B[0m)                                                                                   \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m78 \u001B[0m                                                                                            \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m in \u001B[92mempirical_ntk_ntk_vps\u001B[0m:\u001B[94m50\u001B[0m                                                                      \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m47 \u001B[0m\u001B[2mâ   \u001B[0m\u001B[2m# Since the x1, x2 inputs to empirical_ntk_ntk_vps are batched,\u001B[0m                         \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m48 \u001B[0m\u001B[2mâ   \u001B[0m\u001B[2m# we actually wish to compute the NTK between every pair of data points\u001B[0m                 \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m49 \u001B[0m\u001B[2mâ   \u001B[0m\u001B[2m# between {x1} and {x2}. That's what the vmaps here do.\u001B[0m                                 \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[31mâ± \u001B[0m50 \u001B[2mâ   \u001B[0mresult = vmap(vmap(get_ntk, (\u001B[94mNone\u001B[0m, \u001B[94m0\u001B[0m)), (\u001B[94m0\u001B[0m, \u001B[94mNone\u001B[0m))(x1, x2)                              \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m51 \u001B[0m\u001B[2mâ   \u001B[0m                                                                                        \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m52 \u001B[0m\u001B[2mâ   \u001B[0m\u001B[94mif\u001B[0m compute == \u001B[33m'\u001B[0m\u001B[33mfull\u001B[0m\u001B[33m'\u001B[0m:                                                                   \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m53 \u001B[0m\u001B[2mâ   â   \u001B[0m\u001B[94mreturn\u001B[0m result                                                                       \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[2;33m/home/aris/mambaforge/envs/grab/lib/python3.10/site-packages/torch/_functorch/\u001B[0m\u001B[1;33mvmap.py\u001B[0m:\u001B[94m434\u001B[0m in     \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[92mwrapped\u001B[0m                                                                                          \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m431 \u001B[0m\u001B[2mâ   â   â   â   â   â   â   â    \u001B[0margs_spec, out_dims, randomness, **kwargs)                \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m432 \u001B[0m\u001B[2mâ   â   \u001B[0m                                                                                   \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m433 \u001B[0m\u001B[2mâ   â   \u001B[0m\u001B[2m# If chunk_size is not specified.\u001B[0m                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[31mâ± \u001B[0m434 \u001B[2mâ   â   \u001B[0m\u001B[94mreturn\u001B[0m _flat_vmap(                                                                 \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m435 \u001B[0m\u001B[2mâ   â   â   \u001B[0mfunc, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness,    \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m436 \u001B[0m\u001B[2mâ   â   \u001B[0m)                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m437 \u001B[0m                                                                                           \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[2;33m/home/aris/mambaforge/envs/grab/lib/python3.10/site-packages/torch/_functorch/\u001B[0m\u001B[1;33mvmap.py\u001B[0m:\u001B[94m39\u001B[0m in \u001B[92mfn\u001B[0m   \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m 36 \u001B[0m\u001B[2mâ   \u001B[0m\u001B[1;95m@functools\u001B[0m.wraps(f)                                                                    \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m 37 \u001B[0m\u001B[2mâ   \u001B[0m\u001B[94mdef\u001B[0m \u001B[92mfn\u001B[0m(*args, **kwargs):                                                               \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m 38 \u001B[0m\u001B[2mâ   â   \u001B[0m\u001B[94mwith\u001B[0m torch.autograd.graph.disable_saved_tensors_hooks(message):                    \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[31mâ± \u001B[0m 39 \u001B[2mâ   â   â   \u001B[0m\u001B[94mreturn\u001B[0m f(*args, **kwargs)                                                      \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m 40 \u001B[0m\u001B[2mâ   \u001B[0m\u001B[94mreturn\u001B[0m fn                                                                              \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m 41 \u001B[0m                                                                                           \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m 42 \u001B[0m                                                                                           \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[2;33m/home/aris/mambaforge/envs/grab/lib/python3.10/site-packages/torch/_functorch/\u001B[0m\u001B[1;33mvmap.py\u001B[0m:\u001B[94m619\u001B[0m in     \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[92m_flat_vmap\u001B[0m                                                                                       \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m616 \u001B[0m\u001B[2mâ   \u001B[0mvmap_level = _vmap_increment_nesting(batch_size, randomness)                           \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m617 \u001B[0m\u001B[2mâ   \u001B[0m\u001B[94mtry\u001B[0m:                                                                                   \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m618 \u001B[0m\u001B[2mâ   â   \u001B[0mbatched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, arg   \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[31mâ± \u001B[0m619 \u001B[2mâ   â   \u001B[0mbatched_outputs = func(*batched_inputs, **kwargs)                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m620 \u001B[0m\u001B[2mâ   â   \u001B[0m\u001B[94mreturn\u001B[0m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)    \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m621 \u001B[0m\u001B[2mâ   \u001B[0m\u001B[94mfinally\u001B[0m:                                                                               \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m622 \u001B[0m\u001B[2mâ   â   \u001B[0m_vmap_decrement_nesting()                                                          \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[2;33m/home/aris/mambaforge/envs/grab/lib/python3.10/site-packages/torch/_functorch/\u001B[0m\u001B[1;33mvmap.py\u001B[0m:\u001B[94m434\u001B[0m in     \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[92mwrapped\u001B[0m                                                                                          \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m431 \u001B[0m\u001B[2mâ   â   â   â   â   â   â   â    \u001B[0margs_spec, out_dims, randomness, **kwargs)                \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m432 \u001B[0m\u001B[2mâ   â   \u001B[0m                                                                                   \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m433 \u001B[0m\u001B[2mâ   â   \u001B[0m\u001B[2m# If chunk_size is not specified.\u001B[0m                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[31mâ± \u001B[0m434 \u001B[2mâ   â   \u001B[0m\u001B[94mreturn\u001B[0m _flat_vmap(                                                                 \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m435 \u001B[0m\u001B[2mâ   â   â   \u001B[0mfunc, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness,    \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m436 \u001B[0m\u001B[2mâ   â   \u001B[0m)                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m437 \u001B[0m                                                                                           \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[2;33m/home/aris/mambaforge/envs/grab/lib/python3.10/site-packages/torch/_functorch/\u001B[0m\u001B[1;33mvmap.py\u001B[0m:\u001B[94m39\u001B[0m in \u001B[92mfn\u001B[0m   \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m 36 \u001B[0m\u001B[2mâ   \u001B[0m\u001B[1;95m@functools\u001B[0m.wraps(f)                                                                    \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m 37 \u001B[0m\u001B[2mâ   \u001B[0m\u001B[94mdef\u001B[0m \u001B[92mfn\u001B[0m(*args, **kwargs):                                                               \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m 38 \u001B[0m\u001B[2mâ   â   \u001B[0m\u001B[94mwith\u001B[0m torch.autograd.graph.disable_saved_tensors_hooks(message):                    \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[31mâ± \u001B[0m 39 \u001B[2mâ   â   â   \u001B[0m\u001B[94mreturn\u001B[0m f(*args, **kwargs)                                                      \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m 40 \u001B[0m\u001B[2mâ   \u001B[0m\u001B[94mreturn\u001B[0m fn                                                                              \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m 41 \u001B[0m                                                                                           \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m 42 \u001B[0m                                                                                           \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[2;33m/home/aris/mambaforge/envs/grab/lib/python3.10/site-packages/torch/_functorch/\u001B[0m\u001B[1;33mvmap.py\u001B[0m:\u001B[94m619\u001B[0m in     \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[92m_flat_vmap\u001B[0m                                                                                       \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m616 \u001B[0m\u001B[2mâ   \u001B[0mvmap_level = _vmap_increment_nesting(batch_size, randomness)                           \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m617 \u001B[0m\u001B[2mâ   \u001B[0m\u001B[94mtry\u001B[0m:                                                                                   \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m618 \u001B[0m\u001B[2mâ   â   \u001B[0mbatched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, arg   \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[31mâ± \u001B[0m619 \u001B[2mâ   â   \u001B[0mbatched_outputs = func(*batched_inputs, **kwargs)                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m620 \u001B[0m\u001B[2mâ   â   \u001B[0m\u001B[94mreturn\u001B[0m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)    \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m621 \u001B[0m\u001B[2mâ   \u001B[0m\u001B[94mfinally\u001B[0m:                                                                               \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m622 \u001B[0m\u001B[2mâ   â   \u001B[0m_vmap_decrement_nesting()                                                          \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m in \u001B[92mget_ntk\u001B[0m:\u001B[94m44\u001B[0m                                                                                    \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m41 \u001B[0m\u001B[2mâ   â   \u001B[0m                                                                                    \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m42 \u001B[0m\u001B[2mâ   â   \u001B[0m\u001B[2m# Here's our identity matrix\u001B[0m                                                        \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m43 \u001B[0m\u001B[2mâ   â   \u001B[0mbasis = torch.eye(output.numel(), dtype=output.dtype, device=output.device).view    \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[31mâ± \u001B[0m44 \u001B[2mâ   â   \u001B[0m\u001B[94mreturn\u001B[0m vmap(get_ntk_slice)(basis)                                                   \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m45 \u001B[0m\u001B[2mâ   \u001B[0m                                                                                        \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m46 \u001B[0m\u001B[2mâ   \u001B[0m\u001B[2m# get_ntk(x1, x2) computes the NTK for a single data point x1, x2\u001B[0m                       \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m47 \u001B[0m\u001B[2mâ   \u001B[0m\u001B[2m# Since the x1, x2 inputs to empirical_ntk_ntk_vps are batched,\u001B[0m                         \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[2;33m/home/aris/mambaforge/envs/grab/lib/python3.10/site-packages/torch/_functorch/\u001B[0m\u001B[1;33mvmap.py\u001B[0m:\u001B[94m434\u001B[0m in     \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[92mwrapped\u001B[0m                                                                                          \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m431 \u001B[0m\u001B[2mâ   â   â   â   â   â   â   â    \u001B[0margs_spec, out_dims, randomness, **kwargs)                \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m432 \u001B[0m\u001B[2mâ   â   \u001B[0m                                                                                   \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m433 \u001B[0m\u001B[2mâ   â   \u001B[0m\u001B[2m# If chunk_size is not specified.\u001B[0m                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[31mâ± \u001B[0m434 \u001B[2mâ   â   \u001B[0m\u001B[94mreturn\u001B[0m _flat_vmap(                                                                 \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m435 \u001B[0m\u001B[2mâ   â   â   \u001B[0mfunc, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness,    \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m436 \u001B[0m\u001B[2mâ   â   \u001B[0m)                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m437 \u001B[0m                                                                                           \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[2;33m/home/aris/mambaforge/envs/grab/lib/python3.10/site-packages/torch/_functorch/\u001B[0m\u001B[1;33mvmap.py\u001B[0m:\u001B[94m39\u001B[0m in \u001B[92mfn\u001B[0m   \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m 36 \u001B[0m\u001B[2mâ   \u001B[0m\u001B[1;95m@functools\u001B[0m.wraps(f)                                                                    \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m 37 \u001B[0m\u001B[2mâ   \u001B[0m\u001B[94mdef\u001B[0m \u001B[92mfn\u001B[0m(*args, **kwargs):                                                               \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m 38 \u001B[0m\u001B[2mâ   â   \u001B[0m\u001B[94mwith\u001B[0m torch.autograd.graph.disable_saved_tensors_hooks(message):                    \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[31mâ± \u001B[0m 39 \u001B[2mâ   â   â   \u001B[0m\u001B[94mreturn\u001B[0m f(*args, **kwargs)                                                      \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m 40 \u001B[0m\u001B[2mâ   \u001B[0m\u001B[94mreturn\u001B[0m fn                                                                              \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m 41 \u001B[0m                                                                                           \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m 42 \u001B[0m                                                                                           \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[2;33m/home/aris/mambaforge/envs/grab/lib/python3.10/site-packages/torch/_functorch/\u001B[0m\u001B[1;33mvmap.py\u001B[0m:\u001B[94m619\u001B[0m in     \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[92m_flat_vmap\u001B[0m                                                                                       \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m616 \u001B[0m\u001B[2mâ   \u001B[0mvmap_level = _vmap_increment_nesting(batch_size, randomness)                           \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m617 \u001B[0m\u001B[2mâ   \u001B[0m\u001B[94mtry\u001B[0m:                                                                                   \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m618 \u001B[0m\u001B[2mâ   â   \u001B[0mbatched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, arg   \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[31mâ± \u001B[0m619 \u001B[2mâ   â   \u001B[0mbatched_outputs = func(*batched_inputs, **kwargs)                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m620 \u001B[0m\u001B[2mâ   â   \u001B[0m\u001B[94mreturn\u001B[0m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)    \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m621 \u001B[0m\u001B[2mâ   \u001B[0m\u001B[94mfinally\u001B[0m:                                                                               \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m622 \u001B[0m\u001B[2mâ   â   \u001B[0m_vmap_decrement_nesting()                                                          \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m in \u001B[92mget_ntk_slice\u001B[0m:\u001B[94m37\u001B[0m                                                                              \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m34 \u001B[0m\u001B[2mâ   â   \u001B[0m\u001B[94mdef\u001B[0m \u001B[92mget_ntk_slice\u001B[0m(vec):                                                             \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m35 \u001B[0m\u001B[2mâ   â   â   \u001B[0m\u001B[2m# This computes vec @ J(x2).T\u001B[0m                                                   \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m36 \u001B[0m\u001B[2mâ   â   â   \u001B[0m\u001B[2m# `vec` is some unit vector (a single slice of the Identity matrix)\u001B[0m             \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[31mâ± \u001B[0m37 \u001B[2mâ   â   â   \u001B[0mvjps = vjp_fn(vec)                                                              \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m38 \u001B[0m\u001B[2mâ   â   â   \u001B[0m\u001B[2m# This computes J(X1) @ vjps\u001B[0m                                                    \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m39 \u001B[0m\u001B[2mâ   â   â   \u001B[0m_, jvps = jvp(func_x2, (params,), vjps)                                         \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m40 \u001B[0m\u001B[2mâ   â   â   \u001B[0m\u001B[94mreturn\u001B[0m jvps                                                                     \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[2;33m/home/aris/mambaforge/envs/grab/lib/python3.10/site-packages/torch/_functorch/\u001B[0m\u001B[1;33meager_transforms.p\u001B[0m \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[1;33my\u001B[0m:\u001B[94m325\u001B[0m in \u001B[92mwrapper\u001B[0m                                                                                 \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m 322 \u001B[0m\u001B[2mâ   â   â   â   â   \u001B[0m\u001B[33mf\u001B[0m\u001B[33m'\u001B[0m\u001B[33mas pytree structure of outputs to the function. \u001B[0m\u001B[33m'\u001B[0m                   \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m 323 \u001B[0m\u001B[2mâ   â   â   â   â   \u001B[0m\u001B[33mf\u001B[0m\u001B[33m'\u001B[0m\u001B[33mcotangents: \u001B[0m\u001B[33m{\u001B[0mtreespec_pprint(cotangents_spec)\u001B[33m}\u001B[0m\u001B[33m, \u001B[0m\u001B[33m'\u001B[0m                   \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m 324 \u001B[0m\u001B[2mâ   â   â   â   â   \u001B[0m\u001B[33mf\u001B[0m\u001B[33m'\u001B[0m\u001B[33mprimal output: \u001B[0m\u001B[33m{\u001B[0mtreespec_pprint(primals_out_spec)\u001B[33m}\u001B[0m\u001B[33m'\u001B[0m)                \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[31mâ± \u001B[0m 325 \u001B[2mâ   â   â   \u001B[0mresult = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m 326 \u001B[0m\u001B[2mâ   â   â   â   â   â   â   â   â   \u001B[0mretain_graph=retain_graph, create_graph=create_graph  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m 327 \u001B[0m\u001B[2mâ   â   â   \u001B[0m\u001B[94mreturn\u001B[0m tree_unflatten(result, primals_spec)                                   \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m 328 \u001B[0m                                                                                          \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[2;33m/home/aris/mambaforge/envs/grab/lib/python3.10/site-packages/torch/_functorch/\u001B[0m\u001B[1;33meager_transforms.p\u001B[0m \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[1;33my\u001B[0m:\u001B[94m113\u001B[0m in \u001B[92m_autograd_grad\u001B[0m                                                                          \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m 110 \u001B[0m\u001B[2mâ   â   â   \u001B[0mdiff_outputs, grad_outputs = \u001B[96mzip\u001B[0m(*result)                                     \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m 111 \u001B[0m\u001B[2mâ   \u001B[0m\u001B[94mif\u001B[0m \u001B[96mlen\u001B[0m(diff_outputs) == \u001B[94m0\u001B[0m:                                                            \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m 112 \u001B[0m\u001B[2mâ   â   \u001B[0m\u001B[94mreturn\u001B[0m \u001B[96mtuple\u001B[0m(torch.zeros_like(inp) \u001B[94mfor\u001B[0m inp \u001B[95min\u001B[0m inputs)                             \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[31mâ± \u001B[0m 113 \u001B[2mâ   \u001B[0mgrad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,                 \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m 114 \u001B[0m\u001B[2mâ   â   â   â   â   â   â   â   â     \u001B[0mretain_graph=retain_graph,                          \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m 115 \u001B[0m\u001B[2mâ   â   â   â   â   â   â   â   â     \u001B[0mcreate_graph=create_graph,                          \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m 116 \u001B[0m\u001B[2mâ   â   â   â   â   â   â   â   â     \u001B[0mallow_unused=\u001B[94mTrue\u001B[0m)                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[2;33m/home/aris/mambaforge/envs/grab/lib/python3.10/site-packages/torch/autograd/\u001B[0m\u001B[1;33m__init__.py\u001B[0m:\u001B[94m303\u001B[0m in   \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[92mgrad\u001B[0m                                                                                             \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m                                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m300 \u001B[0m\u001B[2mâ   â   â   â   \u001B[0mallow_unused, accumulate_grad=\u001B[94mFalse\u001B[0m)  \u001B[2m# Calls into the C++ engine to run\u001B[0m   \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m301 \u001B[0m\u001B[2mâ   â   \u001B[0m\u001B[94mreturn\u001B[0m _vmap_internals._vmap(vjp, \u001B[94m0\u001B[0m, \u001B[94m0\u001B[0m, allow_none_pass_through=\u001B[94mTrue\u001B[0m)(grad_outpu   \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m302 \u001B[0m\u001B[2mâ   \u001B[0m\u001B[94melse\u001B[0m:                                                                                  \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m \u001B[31mâ± \u001B[0m303 \u001B[2mâ   â   \u001B[0m\u001B[94mreturn\u001B[0m Variable._execution_engine.run_backward(  \u001B[2m# Calls into the C++ engine to \u001B[0m   \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m304 \u001B[0m\u001B[2mâ   â   â   \u001B[0mt_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,                \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m305 \u001B[0m\u001B[2mâ   â   â   \u001B[0mallow_unused, accumulate_grad=\u001B[94mFalse\u001B[0m)  \u001B[2m# Calls into the C++ engine to run the\u001B[0m   \u001B[31mâ\u001B[0m\n\u001B[31mâ\u001B[0m   \u001B[2m306 \u001B[0m                                                                                           \u001B[31mâ\u001B[0m\n\u001B[31mâ°âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ¯\u001B[0m\n\u001B[1;91mOutOfMemoryError: \u001B[0mCUDA out of memory. Tried to allocate \u001B[1;36m17.53\u001B[0m GiB \u001B[1m(\u001B[0mGPU \u001B[1;36m0\u001B[0m; \u001B[1;36m23.68\u001B[0m GiB total capacity; \u001B[1;36m20.12\u001B[0m GiB \nalready allocated; \u001B[1;36m872.06\u001B[0m MiB free; \u001B[1;36m21.63\u001B[0m GiB reserved in total by PyTorch\u001B[1m)\u001B[0m If reserved memory is >> allocated \nmemory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \nPYTORCH_CUDA_ALLOC_CONF\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">â­âââââââââââââââââââââââââââââââ </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> âââââââââââââââââââââââââââââââââ®</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">85</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">82 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"Failed to load data at once, loading data one by one\"</span>)                       <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">83 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span>...                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">84 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>85 get_0shot_order(model, params, train_dataset)                                               <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">86 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/aris/mambaforge/envs/grab/lib/python3.10/site-packages/torch/utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_contextlib.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">115</span> in   <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">decorate_context</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">112 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">@functools</span>.wraps(func)                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">113 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">decorate_context</span>(*args, **kwargs):                                                 <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">114 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> ctx_factory():                                                                <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>115 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> func(*args, **kwargs)                                                   <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">116 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">117 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> decorate_context                                                                <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">118 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_0shot_order</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">81</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">78 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">79 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(ntk.shape)                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">80 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> torch.cuda.OutOfMemoryError <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> err:                                              <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>81 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> err                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">82 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"Failed to load data at once, loading data one by one\"</span>)                       <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">83 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span>...                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">84 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_0shot_order</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">75</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">72 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span>x = dataset.data.to(device=device).reshape(-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">784</span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># (n, ...)</span>                     <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">73 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span>x = x.float() / <span style=\"color: #0000ff; text-decoration-color: #0000ff\">255</span>                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">74 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>75 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span>ntk = empirical_ntk_ntk_vps(                                                        <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">76 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   â   </span>fnet_single, params, x, x                                                       <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">77 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span>)                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">78 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">empirical_ntk_ntk_vps</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">50</span>                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">47 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Since the x1, x2 inputs to empirical_ntk_ntk_vps are batched,</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">48 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># we actually wish to compute the NTK between every pair of data points</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">49 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># between {x1} and {x2}. That's what the vmaps here do.</span>                                 <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>50 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   </span>result = vmap(vmap(get_ntk, (<span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>)), (<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>))(x1, x2)                              <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">51 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">52 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> compute == <span style=\"color: #808000; text-decoration-color: #808000\">'full'</span>:                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">53 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> result                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/aris/mambaforge/envs/grab/lib/python3.10/site-packages/torch/_functorch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">vmap.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">434</span> in     <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">wrapped</span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">431 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   â   â   â   â   â   â    </span>args_spec, out_dims, randomness, **kwargs)                <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">432 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">433 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># If chunk_size is not specified.</span>                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>434 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> _flat_vmap(                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">435 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   â   </span>func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness,    <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">436 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span>)                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">437 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/aris/mambaforge/envs/grab/lib/python3.10/site-packages/torch/_functorch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">vmap.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">39</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">fn</span>   <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 36 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">@functools</span>.wraps(f)                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 37 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">fn</span>(*args, **kwargs):                                                               <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 38 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.autograd.graph.disable_saved_tensors_hooks(message):                    <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span> 39 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> f(*args, **kwargs)                                                      <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 40 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> fn                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 41 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 42 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/aris/mambaforge/envs/grab/lib/python3.10/site-packages/torch/_functorch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">vmap.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">619</span> in     <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_flat_vmap</span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">616 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   </span>vmap_level = _vmap_increment_nesting(batch_size, randomness)                           <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">617 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">618 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span>batched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, arg   <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>619 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span>batched_outputs = func(*batched_inputs, **kwargs)                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">620 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)    <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">621 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">finally</span>:                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">622 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span>_vmap_decrement_nesting()                                                          <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/aris/mambaforge/envs/grab/lib/python3.10/site-packages/torch/_functorch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">vmap.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">434</span> in     <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">wrapped</span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">431 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   â   â   â   â   â   â    </span>args_spec, out_dims, randomness, **kwargs)                <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">432 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">433 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># If chunk_size is not specified.</span>                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>434 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> _flat_vmap(                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">435 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   â   </span>func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness,    <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">436 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span>)                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">437 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/aris/mambaforge/envs/grab/lib/python3.10/site-packages/torch/_functorch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">vmap.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">39</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">fn</span>   <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 36 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">@functools</span>.wraps(f)                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 37 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">fn</span>(*args, **kwargs):                                                               <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 38 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.autograd.graph.disable_saved_tensors_hooks(message):                    <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span> 39 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> f(*args, **kwargs)                                                      <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 40 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> fn                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 41 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 42 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/aris/mambaforge/envs/grab/lib/python3.10/site-packages/torch/_functorch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">vmap.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">619</span> in     <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_flat_vmap</span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">616 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   </span>vmap_level = _vmap_increment_nesting(batch_size, randomness)                           <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">617 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">618 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span>batched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, arg   <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>619 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span>batched_outputs = func(*batched_inputs, **kwargs)                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">620 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)    <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">621 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">finally</span>:                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">622 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span>_vmap_decrement_nesting()                                                          <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_ntk</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">44</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">41 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">42 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Here's our identity matrix</span>                                                        <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">43 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span>basis = torch.eye(output.numel(), dtype=output.dtype, device=output.device).view    <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>44 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> vmap(get_ntk_slice)(basis)                                                   <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">45 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">46 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># get_ntk(x1, x2) computes the NTK for a single data point x1, x2</span>                       <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">47 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Since the x1, x2 inputs to empirical_ntk_ntk_vps are batched,</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/aris/mambaforge/envs/grab/lib/python3.10/site-packages/torch/_functorch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">vmap.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">434</span> in     <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">wrapped</span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">431 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   â   â   â   â   â   â    </span>args_spec, out_dims, randomness, **kwargs)                <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">432 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">433 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># If chunk_size is not specified.</span>                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>434 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> _flat_vmap(                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">435 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   â   </span>func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness,    <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">436 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span>)                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">437 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/aris/mambaforge/envs/grab/lib/python3.10/site-packages/torch/_functorch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">vmap.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">39</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">fn</span>   <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 36 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">@functools</span>.wraps(f)                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 37 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">fn</span>(*args, **kwargs):                                                               <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 38 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.autograd.graph.disable_saved_tensors_hooks(message):                    <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span> 39 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> f(*args, **kwargs)                                                      <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 40 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> fn                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 41 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 42 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/aris/mambaforge/envs/grab/lib/python3.10/site-packages/torch/_functorch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">vmap.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">619</span> in     <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_flat_vmap</span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">616 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   </span>vmap_level = _vmap_increment_nesting(batch_size, randomness)                           <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">617 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">618 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span>batched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, arg   <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>619 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span>batched_outputs = func(*batched_inputs, **kwargs)                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">620 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)    <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">621 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">finally</span>:                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">622 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span>_vmap_decrement_nesting()                                                          <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_ntk_slice</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">37</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">34 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_ntk_slice</span>(vec):                                                             <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">35 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   â   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># This computes vec @ J(x2).T</span>                                                   <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">36 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   â   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># `vec` is some unit vector (a single slice of the Identity matrix)</span>             <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>37 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   â   </span>vjps = vjp_fn(vec)                                                              <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">38 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   â   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># This computes J(X1) @ vjps</span>                                                    <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">39 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   â   </span>_, jvps = jvp(func_x2, (params,), vjps)                                         <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">40 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> jvps                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/aris/mambaforge/envs/grab/lib/python3.10/site-packages/torch/_functorch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">eager_transforms.p</span> <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">y</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">325</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">wrapper</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 322 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   â   â   â   </span><span style=\"color: #808000; text-decoration-color: #808000\">f'as pytree structure of outputs to the function. '</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 323 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   â   â   â   </span><span style=\"color: #808000; text-decoration-color: #808000\">f'cotangents: {</span>treespec_pprint(cotangents_spec)<span style=\"color: #808000; text-decoration-color: #808000\">}, '</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 324 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   â   â   â   </span><span style=\"color: #808000; text-decoration-color: #808000\">f'primal output: {</span>treespec_pprint(primals_out_spec)<span style=\"color: #808000; text-decoration-color: #808000\">}'</span>)                <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span> 325 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   â   </span>result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 326 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   â   â   â   â   â   â   â   </span>retain_graph=retain_graph, create_graph=create_graph  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 327 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> tree_unflatten(result, primals_spec)                                   <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 328 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/aris/mambaforge/envs/grab/lib/python3.10/site-packages/torch/_functorch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">eager_transforms.p</span> <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">y</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">113</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_autograd_grad</span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 110 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   â   </span>diff_outputs, grad_outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">zip</span>(*result)                                     <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 111 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(diff_outputs) == <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>:                                                            <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 112 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">tuple</span>(torch.zeros_like(inp) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> inp <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> inputs)                             <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span> 113 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   </span>grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,                 <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 114 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   â   â   â   â   â   â   â     </span>retain_graph=retain_graph,                          <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 115 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   â   â   â   â   â   â   â     </span>create_graph=create_graph,                          <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 116 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   â   â   â   â   â   â   â     </span>allow_unused=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/aris/mambaforge/envs/grab/lib/python3.10/site-packages/torch/autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">303</span> in   <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">grad</span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">300 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   â   â   </span>allow_unused, accumulate_grad=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to run</span>   <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">301 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> _vmap_internals._vmap(vjp, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>, allow_none_pass_through=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)(grad_outpu   <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">302 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>303 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> Variable._execution_engine.run_backward(  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to </span>   <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">304 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   â   </span>t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,                <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">305 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â   â   â   </span>allow_unused, accumulate_grad=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to run the</span>   <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">306 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">â</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â°âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ¯</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">OutOfMemoryError: </span>CUDA out of memory. Tried to allocate <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17.53</span> GiB <span style=\"font-weight: bold\">(</span>GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23.68</span> GiB total capacity; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20.12</span> GiB \nalready allocated; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">872.06</span> MiB free; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21.63</span> GiB reserved in total by PyTorch<span style=\"font-weight: bold\">)</span> If reserved memory is &gt;&gt; allocated \nmemory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \nPYTORCH_CUDA_ALLOC_CONF\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from torch.func import grad, grad_and_value, vmap, functional_call, jacrev, vjp, jvp\n",
    "\n",
    "from functorch import make_functional\n",
    "\n",
    "model = nn.Linear(in_dim, num_classes).to(device)\n",
    "\n",
    "fnet, params = make_functional(model)\n",
    "\n",
    "def fnet_single(params, x):\n",
    "    return fnet(params, x.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "# https://pytorch.org/functorch/stable/notebooks/neural_tangent_kernels.html\n",
    "def empirical_ntk_jacobian_contraction(fnet_single, params, x):\n",
    "    # Compute J(x1)\n",
    "    jac = vmap(jacrev(fnet_single), (None, 0))(params, x)\n",
    "    jac = [j.flatten(2) for j in jac]\n",
    "\n",
    "    # Compute J(x1) @ J(x2).T\n",
    "    result = torch.stack(\n",
    "        [torch.einsum('Naf,Mbf->NMab', j1, j2) for j1, j2 in zip(jac, jac)])\n",
    "    result = result.sum(0)\n",
    "    return result\n",
    "\n",
    "def empirical_ntk_ntk_vps(func, params, x1, x2, compute='full'):\n",
    "    def get_ntk(x1, x2):\n",
    "        def func_x1(params):\n",
    "            return func(params, x1)\n",
    "\n",
    "        def func_x2(params):\n",
    "            return func(params, x2)\n",
    "\n",
    "        output, vjp_fn = vjp(func_x1, params)\n",
    "\n",
    "        def get_ntk_slice(vec):\n",
    "            # This computes vec @ J(x2).T\n",
    "            # `vec` is some unit vector (a single slice of the Identity matrix)\n",
    "            vjps = vjp_fn(vec)\n",
    "            # This computes J(X1) @ vjps\n",
    "            _, jvps = jvp(func_x2, (params,), vjps)\n",
    "            return jvps\n",
    "\n",
    "        # Here's our identity matrix\n",
    "        basis = torch.eye(output.numel(), dtype=output.dtype, device=output.device).view(output.numel(), -1)\n",
    "        return vmap(get_ntk_slice)(basis)\n",
    "\n",
    "    # get_ntk(x1, x2) computes the NTK for a single data point x1, x2\n",
    "    # Since the x1, x2 inputs to empirical_ntk_ntk_vps are batched,\n",
    "    # we actually wish to compute the NTK between every pair of data points\n",
    "    # between {x1} and {x2}. That's what the vmaps here do.\n",
    "    result = vmap(vmap(get_ntk, (None, 0)), (0, None))(x1, x2)\n",
    "\n",
    "    if compute == 'full':\n",
    "        return result\n",
    "    if compute == 'trace':\n",
    "        return torch.einsum('NMKK->NM', result)\n",
    "    if compute == 'diagonal':\n",
    "        return torch.einsum('NMKK->NMK', result)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_0shot_order(\n",
    "        model,\n",
    "        params,\n",
    "        dataset,\n",
    "        construct_kernel_matrix: bool = True,\n",
    "        largest_eigval: bool = True,\n",
    "        ascending: bool = True,\n",
    "        device: torch.device = torch.device(\"cuda\"),\n",
    "):\n",
    "    # Try to load data at once\n",
    "    try:\n",
    "        x = dataset.data.to(device=device).reshape(-1, 784)  # (n, ...)\n",
    "        x = x.float() / 255\n",
    "\n",
    "        ntk = empirical_ntk_ntk_vps(\n",
    "            fnet_single, params, x, x\n",
    "        )\n",
    "\n",
    "        print(ntk.shape)\n",
    "    except torch.cuda.OutOfMemoryError as err:\n",
    "        raise err\n",
    "        print(\"Failed to load data at once, loading data one by one\")\n",
    "        ...\n",
    "\n",
    "get_0shot_order(model, params, train_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T01:05:47.975270939Z",
     "start_time": "2023-07-18T01:05:46.927065648Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
